{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns  \n",
    "import requests\n",
    "import time\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "def load_timeline_data(filename):\n",
    "    \"\"\"\n",
    "    Load the JSON timeline data from Google Takeout and returns a DataFrame.\n",
    "    Each record is parsed to extract:\n",
    "      - startTime and endTime (converted to datetime objects)\n",
    "      - duration (in minutes)\n",
    "      - latitude and longitude (parsed from the coordinate string)\n",
    "      - record_type: either 'visit' or 'activity'\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for entry in data:\n",
    "        # Check if record is 'activity' or a 'visit'\n",
    "        if \"activity\" in entry:\n",
    "            # For activity records, extract the \"start\" coordinate.\n",
    "            if \"start\" in entry[\"activity\"]:\n",
    "                coord_str = entry[\"activity\"][\"start\"]\n",
    "            else:\n",
    "                continue\n",
    "            record_type = \"activity\"\n",
    "        elif \"visit\" in entry:\n",
    "            # For visit records,use the coordinate from \"topCandidate\" -> \"placeLocation\"\n",
    "            if \"topCandidate\" in entry[\"visit\"]:\n",
    "                coord_str = entry[\"visit\"][\"topCandidate\"][\"placeLocation\"]\n",
    "            else:\n",
    "                continue\n",
    "            record_type = \"visit\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # The coordinate string is in the format \"geo:lat,lon\". Remove the \"geo:\" prefix.\n",
    "        try:\n",
    "            lat_str, lon_str = coord_str.replace(\"geo:\", \"\").split(\",\")\n",
    "            lat = float(lat_str)\n",
    "            lon = float(lon_str)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Convert startTime and endTime from string to datetime objects.\n",
    "        try:\n",
    "            start_time = datetime.fromisoformat(entry[\"startTime\"])\n",
    "            end_time = datetime.fromisoformat(entry[\"endTime\"])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        \n",
    "        # Calculate duration in minutes.\n",
    "        duration = (end_time - start_time).total_seconds() / 60.0\n",
    "        \n",
    "        records.append({\n",
    "            \"startTime\": start_time,\n",
    "            \"endTime\": end_time,\n",
    "            \"duration_min\": duration,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"record_type\": record_type\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Determine Earliest and Latest Dates in the Data\n",
    "def print_date_range(df):\n",
    "    \"\"\"\n",
    "    Finds and prints the earliest start date and the latest end date.\n",
    "    \"\"\"\n",
    "    earliest = df[\"startTime\"].min()\n",
    "    latest = df[\"endTime\"].max()\n",
    "    print(\"Earliest date in data:\", earliest)\n",
    "    print(\"Latest date in data:\", latest)\n",
    "    return earliest, latest\n",
    "\n",
    "\n",
    "# Clustering Significant Locations using DBSCAN\n",
    "def cluster_locations(df, eps_meters=100, min_samples=3):\n",
    "    \"\"\"\n",
    "    Uses DBSCAN to cluster GPS coordinates of significant locations.\n",
    "    \"\"\"\n",
    "    # Create array of coordinates.\n",
    "    coords = df[['latitude', 'longitude']].to_numpy()\n",
    "    coords_rad = np.radians(coords)\n",
    "    \n",
    "    # Earth's radius in meters.\n",
    "    earth_radius = 6371000.0  \n",
    "    eps = eps_meters / earth_radius\n",
    "    \n",
    "    # Run DBSCAN clustering using the haversine metric.\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(coords_rad)\n",
    "    df['cluster'] = db.labels_\n",
    "    return df, db, eps_meters, min_samples \n",
    "\n",
    "# decided to manually implement colors so that noise could be gray...\n",
    "def get_cluster_colors(df):\n",
    "    \"\"\"\n",
    "    Assigns a fixed set of distinct colors to cluster labels manually.\n",
    "    \"\"\"\n",
    "    manual_colors = {\n",
    "        0: \"red\",  \n",
    "        1: \"blue\",  \n",
    "        2: \"green\",  \n",
    "        3: \"purple\", \n",
    "        4: \"orange\", \n",
    "        5: \"brown\",  \n",
    "        6: \"pink\", \n",
    "        7: \"black\",  \n",
    "        8: \"teal\",  \n",
    "        9: \"yellow\",  \n",
    "        -1: \"gray\"    \n",
    "    }\n",
    "\n",
    "    # Add additional fallback colors if more clusters appear than expected\n",
    "    unique_clusters = df['cluster'].unique()\n",
    "    for cluster in unique_clusters:\n",
    "        if cluster not in manual_colors:\n",
    "            manual_colors[cluster] = \"black\"  # fallback: black\n",
    "\n",
    "    return manual_colors\n",
    "\n",
    "\n",
    "# Visualization of Clusters\n",
    "def plot_clusters(df, eps_meters=None, min_samples=None, cluster_colors=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster in df['cluster'].unique():\n",
    "        cluster_data = df[df['cluster'] == cluster]\n",
    "        color = cluster_colors.get(cluster, 'gray')\n",
    "        label = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        plt.scatter(cluster_data['longitude'], cluster_data['latitude'],    \n",
    "            c=color, alpha=0.6, label=label)\n",
    "\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.title(\"DBSCAN Clusters of Significant Locations\")\n",
    "    plt.legend()\n",
    "    if eps_meters is not None and min_samples is not None:\n",
    "        plt.text(0.01, 0.01, f\"eps = {eps_meters} meters\\nmin_samples = {min_samples}\",\n",
    "                 transform=plt.gca().transAxes, fontsize=10,\n",
    "                 bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "\n",
    "# Label Clusters Using Google Places API\n",
    "def label_clusters(df, api_key, radius=200, sleep_time=1.0):\n",
    "    \"\"\"\n",
    "    Assign a label to each cluster based on the Google Places API.\n",
    "    - Only queries once per non-noise cluster (at centroid)\n",
    "    - Returns the labeled DataFrame, cluster_id → label dict, and place metadata\n",
    "    \"\"\"\n",
    "    cluster_labels = {}\n",
    "    place_results = {}\n",
    "\n",
    "    print(\"Querying Google Places API for cluster labels...\")\n",
    "\n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            cluster_labels[cluster_id] = \"Noise\"\n",
    "            continue\n",
    "\n",
    "        top_places = [] \n",
    "\n",
    "        cluster_df = df[df['cluster'] == cluster_id]\n",
    "        centroid_lat = cluster_df['latitude'].mean()\n",
    "        centroid_lon = cluster_df['longitude'].mean()\n",
    "\n",
    "        url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "        params = {\n",
    "            \"location\": f\"{centroid_lat},{centroid_lon}\",\n",
    "            \"radius\": radius,\n",
    "            \"key\": api_key\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            label = \"Unknown\"\n",
    "            if data[\"status\"] == \"OK\" and data[\"results\"]:\n",
    "                print(f\"\\nCluster {cluster_id}: Top 5 Google Places Results\")\n",
    "                for i, place in enumerate(data[\"results\"][:5]):\n",
    "                    name = place.get(\"name\", \"Unnamed\")\n",
    "                    types = place.get(\"types\", [\"Unknown\"])\n",
    "                    rating = place.get(\"rating\", None)\n",
    "                    user_ratings_total = place.get(\"user_ratings_total\", None)\n",
    "\n",
    "                    print(f\"  {i+1}. {name} → {types}\")\n",
    "                    top_places.append({\n",
    "                        \"cluster_id\": cluster_id,\n",
    "                        \"rank\": i + 1,\n",
    "                        \"place_name\": name,\n",
    "                        \"types\": types,\n",
    "                        \"rating\": rating,\n",
    "                        \"user_ratings_total\": user_ratings_total\n",
    "                    })\n",
    "                    for t in types:\n",
    "                        if t not in [\"locality\", \"political\", \"point_of_interest\", \"establishment\", \"premise\"]:\n",
    "                            if label == \"Unknown\":\n",
    "                                label = t\n",
    "                            break\n",
    "                            \n",
    "                if label == \"Unknown\":\n",
    "                    print(\"No specific type found - defaulting to unknown!\")\n",
    "            else:\n",
    "                print(f\"Cluster {cluster_id}: No results (status: {data.get('status', 'N/A')})\")\n",
    "            \n",
    "            cluster_labels[cluster_id] = label\n",
    "            place_results[cluster_id] = top_places  # save top 5 results since first was mostly like city name and not informative enough\n",
    "\n",
    "        except Exception as e:\n",
    "            label = \"Error\"\n",
    "            print(f\"Cluster {cluster_id}: API error - {e}\")\n",
    "            place_results[cluster_id] = {\"status\": \"EXCEPTION\", \"error\": str(e)}\n",
    "\n",
    "        cluster_labels[cluster_id] = label\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # Assign labels back to DataFrame\n",
    "    df['cluster_label'] = df['cluster'].map(cluster_labels)\n",
    "    return df, cluster_labels, place_results\n",
    "\n",
    "\n",
    "def plot_labeled_clusters(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label in df['cluster_label'].unique():\n",
    "        label_df = df[df['cluster_label'] == label]\n",
    "        plt.scatter(label_df['longitude'], label_df['latitude'], label=label, alpha=0.6)\n",
    "\n",
    "    plt.title(\"Clusters with Inferred Location Types\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Zoom in to regions to see more clusters there\n",
    "def plot_zoomed_regions(df, regions_dict, cluster_colors):\n",
    "    for region_name, bounds in regions_dict.items():\n",
    "        zoom_df = df[\n",
    "            (df['latitude'] >= bounds['lat_min']) & (df['latitude'] <= bounds['lat_max']) &\n",
    "            (df['longitude'] >= bounds['lon_min']) & (df['longitude'] <= bounds['lon_max'])\n",
    "        ]\n",
    "        \n",
    "        if zoom_df.empty:\n",
    "            print(f\"No data in {region_name}\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Now loop over cluster labels\n",
    "        for cluster_label in zoom_df['cluster'].unique():\n",
    "            cluster_data = zoom_df[zoom_df['cluster'] == cluster_label]\n",
    "            color = cluster_colors.get(cluster_label, 'gray')\n",
    "\n",
    "            label = \"Noise\" if cluster_label == -1 else f\"Cluster {cluster_label}\"\n",
    "            plt.scatter(cluster_data['longitude'], cluster_data['latitude'],    \n",
    "                c=color, alpha=0.6, label=label)\n",
    "        \n",
    "        plt.title(f\"Zoomed View: {region_name}\")\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name\n",
    "filename = \"location_history_newer.json\"\n",
    "\n",
    "# Load and preprocess the data.\n",
    "df = load_timeline_data(filename)\n",
    "print(\"Sample loaded data:\")\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Total number of records loaded:\", len(df))\n",
    "print(\"Total number of data points (before filtering):\", df.shape[0])\n",
    "\n",
    "\n",
    "# Print earliest and latest dates in the dataset.\n",
    "earliest, latest = print_date_range(df)\n",
    "\n",
    "\n",
    "# Filter to keep only significant records.\n",
    "# Here, we assume that a record is 'significant' if the duration is over 5 minutes OR it is a visit!\n",
    "significant_df = df[(df['duration_min'] > 5) | (df['record_type'] == 'visit')]\n",
    "print(\"Significant records for clustering (sample):\")\n",
    "print(significant_df.head(), \"\\n\")\n",
    "print(\"Number of significant records (after filtering):\", significant_df.shape[0])\n",
    "\n",
    "\n",
    "# do clustering on the significant data.\n",
    "# clustered_df, db_model = cluster_locations(significant_df, eps_meters=10000, min_samples=10)\n",
    "clustered_df, db, eps_val, min_samples_val = cluster_locations(df, eps_meters=500, min_samples=10)\n",
    "print(\"Clustered data (sample):\")\n",
    "print(clustered_df.head(), \"\\n\")\n",
    "cluster_colors = get_cluster_colors(clustered_df)\n",
    "\n",
    "\n",
    "# Print size of each cluster\n",
    "cluster_sizes = clustered_df['cluster'].value_counts().sort_index()\n",
    "print(\"Cluster sizes (including noise as -1):\")\n",
    "print(cluster_sizes, \"\\n\")\n",
    "\n",
    "\n",
    "# Visualize the clusters.\n",
    "# plot_clusters(clustered_df)\n",
    "# plot_clusters(clustered_df, eps_meters=eps_val, min_samples=min_samples_val)\n",
    "plot_clusters(clustered_df, eps_meters=eps_val, min_samples=min_samples_val, cluster_colors=cluster_colors)\n",
    "\n",
    "\n",
    "\n",
    "# Label each cluster using the Google Places API.\n",
    "# Replace \"YOUR_KEY\" with your actual API key.\n",
    "api_key = \"YOUR-KEY\" # removed...\n",
    "labeled_df, cluster_labels, place_results = label_clusters(clustered_df, api_key)\n",
    "print(\"Assigned cluster labels:\")\n",
    "print(cluster_labels, \"\\n\")\n",
    "\n",
    "# Flatten and save place metadata\n",
    "rows = []\n",
    "for cluster_id, places in place_results.items():\n",
    "    for p in places:\n",
    "        rows.append({\n",
    "            \"cluster_id\": p[\"cluster_id\"],\n",
    "            \"rank\": p[\"rank\"],\n",
    "            \"place_name\": p[\"place_name\"],\n",
    "            \"place_types\": \", \".join(p[\"types\"]),\n",
    "            \"rating\": p[\"rating\"],\n",
    "            \"user_ratings_total\": p[\"user_ratings_total\"]\n",
    "        })\n",
    "\n",
    "# Save to CSV\n",
    "places_df = pd.DataFrame(rows)\n",
    "places_df.to_csv(\"cluster_top5_places.csv\", index=False)\n",
    "print(\"Saved cluster_top5_places.csv\")\n",
    "\n",
    "label_df = pd.DataFrame([\n",
    "    {\"cluster_id\": cid, \"label\": label}\n",
    "    for cid, label in cluster_labels.items()\n",
    "])\n",
    "label_df.to_csv(\"cluster_labels.csv\", index=False)\n",
    "print(\"Saved cluster_labels.csv\")\n",
    "\n",
    "# Manually added regions to zoom into based on my data - redacted for safety\n",
    "zoom_regions = {\n",
    "\"Region 1 (North)\": {\"lat_min\": 5, \"lat_max\": 5, \"lon_min\": 5, \"lon_max\": 5},\n",
    "    \"Region 2 (West)\": {\n",
    "    \"lat_min\": 5, \"lat_max\": 5,\n",
    "    \"lon_min\": 5, \"lon_max\": 5\n",
    "},\n",
    "\"Region 3 (Central)\": {\n",
    "    \"lat_min\": 5, \"lat_max\": 5,\n",
    "    \"lon_min\": 5, \"lon_max\": 5\n",
    "}\n",
    "}\n",
    "\n",
    "# plot_zoomed_regions(clustered_df, zoom_regions)\n",
    "plot_zoomed_regions(clustered_df, zoom_regions, cluster_colors)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot my clusters on folium, real-world map for visualization!\n",
    "def plot_clusters_on_map(df, cluster_colors):\n",
    "    # Use the center of your data as the map center\n",
    "    center_lat = df['latitude'].mean()\n",
    "    center_lon = df['longitude'].mean()\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=11)\n",
    "\n",
    "    marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        cluster_id = row['cluster']\n",
    "        label = f\"Cluster {cluster_id}\" if cluster_id != -1 else \"Noise\"\n",
    "        color = cluster_colors.get(cluster_id, 'gray')\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=(row['latitude'], row['longitude']),\n",
    "            radius=4,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.6,\n",
    "            popup=f\"{label}\"\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "    return m\n",
    "\n",
    "filtered_df = labeled_df[labeled_df['cluster'] != -1]\n",
    "map_view = plot_clusters_on_map(filtered_df, cluster_colors)\n",
    "map_view.save(\"clustered_map.html\")\n",
    "\n",
    "map_view\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML-IA-class)",
   "language": "python",
   "name": "ml-ia-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
